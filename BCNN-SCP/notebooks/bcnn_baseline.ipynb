{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Seaborn style for visualization\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class SpatialCovariance(ABC):\n",
    "    def __call__(self, w, h):\n",
    "        xs = torch.linspace(0, 1, w)\n",
    "        ys = torch.linspace(0, 1, h)\n",
    "        points = [torch.tensor([x, y], dtype=torch.float) for x in xs for y in ys]\n",
    "        covar = torch.zeros(len(points), len(points))\n",
    "        for i in range(len(points)):\n",
    "            for j in range(i, len(points)):\n",
    "                x = points[i]\n",
    "                y = points[j]\n",
    "                covar[i, j] = self.kernel(x, y)\n",
    "                covar[j, i] = covar[i, j]\n",
    "        return covar\n",
    "\n",
    "    @abstractmethod\n",
    "    def kernel(self, x, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class RBFCovariance(SpatialCovariance):\n",
    "    def __init__(self, a, l):\n",
    "        self.a = a\n",
    "        self.l = l\n",
    "\n",
    "    def kernel(self, x, y):\n",
    "        diff = torch.norm(x - y)\n",
    "        return self.a * torch.exp(-(diff**2) / (2 * self.l**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and dataloader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "dataset = datasets.MNIST(root='../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into train and validation subsets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize a batch of images\n",
    "def visualize_images(dataloader, title=\"Images\"):\n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(dataloader))\n",
    "\n",
    "    # Denormalize images for visualization\n",
    "    images = images * 0.5 + 0.5  # Reverse normalization to [0, 1]\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(8, 4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for img, label, ax in zip(images[:8], labels[:8], axes):\n",
    "        ax.imshow(img.squeeze(), cmap='gray')  # Display the single-channel image\n",
    "        ax.set_title(f\"Label: {label.item()}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize train, validation, and test batches\n",
    "visualize_images(train_loader, title=\"Visualise images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline B-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_DIV(mu_p, sig_p, mu_q, sig_q):\n",
    "    kl = 0\n",
    "    kernel_size = sig_q.shape[1]\n",
    "    blocks = sig_q.shape[0]\n",
    "    n = sig_q.shape[1]\n",
    "    sig_p_inv = torch.linalg.pinv(sig_p)\n",
    "    term_1_2 = torch.logdet(sig_p)\n",
    "    for i in range(blocks):\n",
    "        term_1_1 = torch.logdet(sig_q[i,:,:])\n",
    "        term_1 =  term_1_1 - term_1_2\n",
    "        term_3 = torch.trace(sig_p_inv @ sig_q[i,:,:])\n",
    "        mu_diff = (mu_q[kernel_size*i:kernel_size*(i+1)] - mu_p[kernel_size*i:kernel_size*(i+1)])\n",
    "        loss = (term_1 + n - term_3 - (mu_diff).T @ sig_p_inv @ (mu_diff))\n",
    "        # print(\"term11\", term_1_1)\n",
    "        # print(\"term12\", term_1_2)\n",
    "        # print(\"term3\", term_3)\n",
    "        # print(\"sig_p_inv\", sig_p_inv)\n",
    "        kl += loss\n",
    "\n",
    "    return -0.5*kl\n",
    "\n",
    "\n",
    "class RBF(gpytorch.kernels.RBFKernel):\n",
    "    def __init__(self, a, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.outputscale = a\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "        # covar = super().forward(x1, x2, **params)\n",
    "        covar = torch.eye(x1.shape[0])\n",
    "        return covar * self.outputscale\n",
    "\n",
    "class BBBConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, priors=None, num_samples=1):\n",
    "\n",
    "        super(BBBConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_shape = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.kernel_size = self.kernel_shape[0] * self.kernel_shape[1]\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = 1\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        prior_mu = torch.zeros(self.kernel_size*in_channels*out_channels,)\n",
    "        rbf_kernel = RBFCovariance(1, 1)\n",
    "        prior_covariance_matrix = rbf_kernel(self.kernel_shape[0], self.kernel_shape[1])\n",
    "\n",
    "        grid_size = self.kernel_shape[0]\n",
    "        x = torch.linspace(0, 1, grid_size)\n",
    "        y = torch.linspace(0, 1, grid_size)\n",
    "        xx, yy = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "        self.points = torch.stack([xx.ravel(), yy.ravel()], dim=-1)\n",
    "        self.counter = 0\n",
    "\n",
    "        if priors is None:\n",
    "            priors = {\n",
    "                'prior_mu': prior_mu,\n",
    "                'prior_cov': prior_covariance_matrix\n",
    "            }\n",
    "\n",
    "        # prior mean and convariance\n",
    "\n",
    "        self.prior_mu = priors['prior_mu']\n",
    "        self.prior_cov = priors['prior_cov']\n",
    "\n",
    "        # kernel for weight paramters (W_mu and W_sigma)\n",
    "        # a and l are learnable here\n",
    "\n",
    "        self.a = nn.Parameter(1+torch.rand(in_channels*out_channels))\n",
    "        self.l = nn.Parameter(1+torch.rand(in_channels*out_channels))\n",
    "\n",
    "        self.W_mu = nn.Parameter(torch.rand(in_channels*out_channels*self.kernel_size))\n",
    "        self.sampled_weights = torch.rand((self.num_samples,) + self.W_mu.shape)\n",
    "\n",
    "    def forward(self, input, sample=True):\n",
    "        # (B,S,C,H,W)\n",
    "        if self.training or sample:\n",
    "            # W_mu, a and l are learnable parameters\n",
    "            self.sample_weights()\n",
    "\n",
    "            # now we have sampled \"num_samples\" at once\n",
    "            # we iterate through the S dimension and run each set of weights for each sample\n",
    "            # it is guarenteed that self.num_samples matches with S (except for first layer, where input will have S = 1)\n",
    "            S = input.shape[1]\n",
    "            outputs = []\n",
    "            for i in range(self.num_samples):\n",
    "                weight = self.sampled_weights[i,:].view(self.out_channels, self.in_channels, self.kernel_shape[0], self.kernel_shape[1])\n",
    "\n",
    "                input_index = 0 if S == 1 else i\n",
    "                outputs.append(F.conv2d(input[:,input_index,:,:,:], weight, None, self.stride, self.padding, self.dilation, self.groups))\n",
    "            return torch.stack(outputs, dim=1)\n",
    "        else:\n",
    "            weight = self.W_mu.view(self.out_channels, self.in_channels, self.kernel_shape[0], self.kernel_shape[1])\n",
    "            return F.conv2d(input[:,0,:,:,:], weight, None, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def kl_loss(self):\n",
    "        return KL_DIV(self.prior_mu, self.prior_cov, self.W_mu, self.W_cov)\n",
    "\n",
    "    def sample_weights(self):\n",
    "        blocks = []\n",
    "        for i in range(self.in_channels*self.out_channels):\n",
    "            # different a and l for different filter (x,y)\n",
    "            rbf_kernel = RBFCovariance(self.a[i], self.l[i])\n",
    "\n",
    "            filterwise_covariance_matrix = rbf_kernel(self.kernel_shape[0], self.kernel_shape[1])\n",
    "            jitter = 1e-6 * torch.eye(filterwise_covariance_matrix.size(0))\n",
    "            filterwise_covariance_matrix += jitter\n",
    "\n",
    "            # samples \"num_samples\" weights at once\n",
    "            mvn = torch.distributions.MultivariateNormal(self.W_mu[self.kernel_size*i : self.kernel_size*i+self.kernel_size], filterwise_covariance_matrix)\n",
    "            self.sampled_weights[:, self.kernel_size*i : self.kernel_size*i+self.kernel_size] = mvn.sample((self.num_samples,))\n",
    "\n",
    "            self.counter+=1\n",
    "\n",
    "            blocks.append(filterwise_covariance_matrix)\n",
    "\n",
    "        self.W_cov = torch.stack(blocks) # concats only the blocks of the block diagonal covariance matrix\n",
    "\n",
    "class BayesianCNN(nn.Module):\n",
    "    def __init__(self, num_samples=1):\n",
    "        super(BayesianCNN, self).__init__()\n",
    "        self.conv1 = BBBConv2d(1, 32, kernel_size=3, stride=1, padding=1, num_samples=num_samples)\n",
    "        self.conv2 = BBBConv2d(32, 64, kernel_size=3, stride=1, padding=1, num_samples=num_samples)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = x_in.unsqueeze(1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        y=[]\n",
    "        for i in range(self.num_samples):\n",
    "            y.append(self.pool(x[:,i,:,:,:]))\n",
    "        y = torch.stack(y, dim=1)\n",
    "        y = y.view(y.size(0), y.size(1), -1)  # Flatten\n",
    "        y = torch.relu(self.fc1(y))\n",
    "        y = self.fc2(y)\n",
    "        y = F.log_softmax(y, dim=2)\n",
    "\n",
    "        kl = 0.0\n",
    "        for module in self.children():\n",
    "            if hasattr(module, 'kl_loss'):\n",
    "                module_kl_loss = module.kl_loss()\n",
    "                kl = kl + module_kl_loss\n",
    "\n",
    "        return y, kl\n",
    "\n",
    "class NonBayesianCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonBayesianCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100, 32, 7, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=BBBConv2d(1,32,3,num_samples=100)\n",
    "model(torch.rand(64, 1, 1, 9, 9)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/92/fmjkh__50rq2yj5c9sl265y80000gn/T/ipykernel_9622/642770217.py:13: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3687.)\n",
      "  loss = (term_1 + n - term_3 - (mu_diff).T @ sig_p_inv @ (mu_diff))\n"
     ]
    }
   ],
   "source": [
    "model = BayesianCNN(num_samples=100)\n",
    "out = model(torch.rand(64, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 100, 10]), tensor(272240.8438, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape, out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        # with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        #     tepoch.set_description(\"Evaluating\")\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            if len(outputs) == 2: outputs = outputs[0]\n",
    "\n",
    "            # Compute predictions and update accuracy metrics\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Display current accuracy in the progress bar\n",
    "            accuracy = 100 * correct / total\n",
    "    accuracy = 100 * correct / total  # Overall accuracy\n",
    "    return accuracy\n",
    "\n",
    "def train_validate_and_evaluate(model, optimizer, device, epochs, train_loader, test_loader, val_loader, criterion=None, use_kl=True):\n",
    "    # Total number of batches across all epochs\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Single progress bar for all epochs\n",
    "    with tqdm(total=total_steps, unit=\"batch\") as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_combined_loss = 0.0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # # One-hot encode labels\n",
    "                # one_hot_labels = F.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                if len(outputs) == 2:\n",
    "                    logits, kl_loss = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "\n",
    "                # # Apply softmax to logits\n",
    "                # softmax_outputs = F.softmax(logits, dim=1)\n",
    "\n",
    "                # Cross entropy loss\n",
    "                if len(logits.shape) == 3:\n",
    "                    logits = logits.permute(0, 2, 1) # (batch_size, num_samples, num_classes) -> (batch_size, num_classes, num_samples)\n",
    "                    labels = labels.unsqueeze(-1) # (batch_size, ) -> (batch_size, 1)\n",
    "                    labels = labels.expand(-1, logits.shape[-1]) # (batch_size, 1) -> (batch_size, num_samples)\n",
    "                    cross_entropy_loss = F.cross_entropy(logits, labels, reduction='none') # (batch_size, num_samples)\n",
    "                    cross_entropy_loss = cross_entropy_loss.mean(-1) # average over samples\n",
    "                    cross_entropy_loss = cross_entropy_loss.sum() # sum over minibatch\n",
    "                else:\n",
    "                    cross_entropy_loss = F.cross_entropy(logits, labels, reduction='sum')\n",
    "\n",
    "                combined_loss = 0.0\n",
    "                if criterion: combined_loss += cross_entropy_loss\n",
    "                if use_kl: combined_loss += kl_loss\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                combined_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update metrics\n",
    "                total_combined_loss += combined_loss.item()\n",
    "\n",
    "                # Update the progress bar dynamically\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "                pbar.set_postfix(combined_loss=combined_loss.item())\n",
    "                pbar.update(1)\n",
    "\n",
    "            avg_combined_loss = total_combined_loss / len(train_loader)\n",
    "            losses.append(avg_combined_loss)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Combined Loss: {avg_combined_loss:.4f}\")\n",
    "            val_acc = evaluate(model, val_loader, device)\n",
    "            val_accuracies.append(val_acc)\n",
    "            print(f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "    print(f\"Testing complete. Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    # Create a line plot\n",
    "    sns.lineplot(x=np.arange(epochs), y=losses)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs. Epoch\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Create a line plot\n",
    "    sns.lineplot(x=np.arange(epochs), y=val_accuracies)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Val Accuracy\")\n",
    "    plt.title(\"Val Accuracy vs. Epoch\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Non Bayesian CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dbd0cd96f1489b8a374636f32759ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_validate_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_kl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 45\u001b[0m, in \u001b[0;36mtrain_validate_and_evaluate\u001b[0;34m(model, optimizer, device, epochs, train_loader, test_loader, val_loader, criterion, use_kl)\u001b[0m\n\u001b[1;32m     39\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# # One-hot encode labels\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# one_hot_labels = F.one_hot(labels, num_classes=10).float()\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     47\u001b[0m     logits, kl_loss \u001b[38;5;241m=\u001b[39m outputs\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 166\u001b[0m, in \u001b[0;36mNonBayesianCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    165\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    167\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = NonBayesianCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_validate_and_evaluate(model, optimizer, device, epochs, train_loader, test_loader, val_loader, criterion=criterion, use_kl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Bayesian CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5db9713267f4c989c7f687e663e6405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_validate_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_kl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 71\u001b[0m, in \u001b[0;36mtrain_validate_and_evaluate\u001b[0;34m(model, optimizer, device, epochs, train_loader, test_loader, val_loader, criterion, use_kl)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 71\u001b[0m \u001b[43mcombined_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = BayesianCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_validate_and_evaluate(model, optimizer, device, epochs, train_loader, test_loader, val_loader, criterion=criterion, use_kl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
